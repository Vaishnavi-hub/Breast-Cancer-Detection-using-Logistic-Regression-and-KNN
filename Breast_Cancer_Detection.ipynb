{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYDEMxIzxSak"
   },
   "source": [
    "**Breast Cancer Detection using Logistic Regression and KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0URPxW3wxXj6"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vkxI3qE_x7yQ"
   },
   "outputs": [],
   "source": [
    "# getting the dataset\n",
    "breast_cancer = sklearn.datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsBtIA8RzG38",
    "outputId": "8852594e-f891-43d6-df20-2860c8d89e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
      "        1.189e-01],\n",
      "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
      "        8.902e-02],\n",
      "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
      "        8.758e-02],\n",
      "       ...,\n",
      "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
      "        7.820e-02],\n",
      "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
      "        1.240e-01],\n",
      "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
      "        7.039e-02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), 'target_names': array(['malignant', 'benign'], dtype='<U9'), 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.', 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error',\n",
      "       'fractal dimension error', 'worst radius', 'worst texture',\n",
      "       'worst perimeter', 'worst area', 'worst smoothness',\n",
      "       'worst compactness', 'worst concavity', 'worst concave points',\n",
      "       'worst symmetry', 'worst fractal dimension'], dtype='<U23'), 'filename': 'C:\\\\Users\\\\VAISHU\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\breast_cancer.csv'}\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer)\n",
    "# class labels (0 - Malignant, 1 - Benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "25lVPZzDzhsq"
   },
   "outputs": [],
   "source": [
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJuxdvGY0C26",
    "outputId": "2cba64b4-f4a7-484c-9583-bb4e6e1f365a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nJc2bos0KZB",
    "outputId": "9b219d24-56a1-4c8d-d33d-66b69cd6ef4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-T-92ROl0OX4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C7Q-KyaA0U8s"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(breast_cancer.data, columns = breast_cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "xQOWmY_o0fu8",
    "outputId": "51468e8e-4ad6-4858-ea4b-a88cd4c217d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XT1a-aHx0ii3"
   },
   "outputs": [],
   "source": [
    "df['class'] = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "id": "MpD_vkEU0rwL",
    "outputId": "b579a9dc-00d9-4863-8131-5121e1ac986b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.03299</td>\n",
       "      <td>0.03323</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.09954</td>\n",
       "      <td>0.06606</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.5609</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.20650</td>\n",
       "      <td>0.11180</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.09938</td>\n",
       "      <td>0.05364</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.21280</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.1651</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.16390</td>\n",
       "      <td>0.07364</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>0.6577</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.07395</td>\n",
       "      <td>0.05259</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.17220</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>0.4233</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.14790</td>\n",
       "      <td>0.09498</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.04781</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1773</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.03110</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.02076</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.1324</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.20770</td>\n",
       "      <td>0.09756</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.10970</td>\n",
       "      <td>0.08632</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.09170</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.08783</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.07731</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.16830</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.6110</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.07953</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.990         10.38          122.80     1001.0          0.11840   \n",
       "1        20.570         17.77          132.90     1326.0          0.08474   \n",
       "2        19.690         21.25          130.00     1203.0          0.10960   \n",
       "3        11.420         20.38           77.58      386.1          0.14250   \n",
       "4        20.290         14.34          135.10     1297.0          0.10030   \n",
       "5        12.450         15.70           82.57      477.1          0.12780   \n",
       "6        18.250         19.98          119.60     1040.0          0.09463   \n",
       "7        13.710         20.83           90.20      577.9          0.11890   \n",
       "8        13.000         21.82           87.50      519.8          0.12730   \n",
       "9        12.460         24.04           83.97      475.9          0.11860   \n",
       "10       16.020         23.24          102.70      797.8          0.08206   \n",
       "11       15.780         17.89          103.60      781.0          0.09710   \n",
       "12       19.170         24.80          132.40     1123.0          0.09740   \n",
       "13       15.850         23.95          103.70      782.7          0.08401   \n",
       "14       13.730         22.61           93.60      578.3          0.11310   \n",
       "15       14.540         27.54           96.73      658.8          0.11390   \n",
       "16       14.680         20.13           94.74      684.5          0.09867   \n",
       "17       16.130         20.68          108.10      798.8          0.11700   \n",
       "18       19.810         22.15          130.00     1260.0          0.09831   \n",
       "19       13.540         14.36           87.46      566.3          0.09779   \n",
       "20       13.080         15.71           85.63      520.0          0.10750   \n",
       "21        9.504         12.44           60.34      273.9          0.10240   \n",
       "22       15.340         14.26          102.50      704.4          0.10730   \n",
       "23       21.160         23.04          137.20     1404.0          0.09428   \n",
       "24       16.650         21.38          110.00      904.6          0.11210   \n",
       "25       17.140         16.40          116.00      912.7          0.11860   \n",
       "26       14.580         21.53           97.41      644.8          0.10540   \n",
       "27       18.610         20.25          122.10     1094.0          0.09440   \n",
       "28       15.300         25.27          102.40      732.4          0.10820   \n",
       "29       17.570         15.05          115.00      955.1          0.09847   \n",
       "\n",
       "    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.27760         0.30010              0.14710         0.2419   \n",
       "1            0.07864         0.08690              0.07017         0.1812   \n",
       "2            0.15990         0.19740              0.12790         0.2069   \n",
       "3            0.28390         0.24140              0.10520         0.2597   \n",
       "4            0.13280         0.19800              0.10430         0.1809   \n",
       "5            0.17000         0.15780              0.08089         0.2087   \n",
       "6            0.10900         0.11270              0.07400         0.1794   \n",
       "7            0.16450         0.09366              0.05985         0.2196   \n",
       "8            0.19320         0.18590              0.09353         0.2350   \n",
       "9            0.23960         0.22730              0.08543         0.2030   \n",
       "10           0.06669         0.03299              0.03323         0.1528   \n",
       "11           0.12920         0.09954              0.06606         0.1842   \n",
       "12           0.24580         0.20650              0.11180         0.2397   \n",
       "13           0.10020         0.09938              0.05364         0.1847   \n",
       "14           0.22930         0.21280              0.08025         0.2069   \n",
       "15           0.15950         0.16390              0.07364         0.2303   \n",
       "16           0.07200         0.07395              0.05259         0.1586   \n",
       "17           0.20220         0.17220              0.10280         0.2164   \n",
       "18           0.10270         0.14790              0.09498         0.1582   \n",
       "19           0.08129         0.06664              0.04781         0.1885   \n",
       "20           0.12700         0.04568              0.03110         0.1967   \n",
       "21           0.06492         0.02956              0.02076         0.1815   \n",
       "22           0.21350         0.20770              0.09756         0.2521   \n",
       "23           0.10220         0.10970              0.08632         0.1769   \n",
       "24           0.14570         0.15250              0.09170         0.1995   \n",
       "25           0.22760         0.22290              0.14010         0.3040   \n",
       "26           0.18680         0.14250              0.08783         0.2252   \n",
       "27           0.10660         0.14900              0.07731         0.1697   \n",
       "28           0.16970         0.16830              0.08751         0.1926   \n",
       "29           0.11570         0.09875              0.07953         0.1739   \n",
       "\n",
       "    mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                  0.07871  ...          17.33           184.60      2019.0   \n",
       "1                  0.05667  ...          23.41           158.80      1956.0   \n",
       "2                  0.05999  ...          25.53           152.50      1709.0   \n",
       "3                  0.09744  ...          26.50            98.87       567.7   \n",
       "4                  0.05883  ...          16.67           152.20      1575.0   \n",
       "5                  0.07613  ...          23.75           103.40       741.6   \n",
       "6                  0.05742  ...          27.66           153.20      1606.0   \n",
       "7                  0.07451  ...          28.14           110.60       897.0   \n",
       "8                  0.07389  ...          30.73           106.20       739.3   \n",
       "9                  0.08243  ...          40.68            97.65       711.4   \n",
       "10                 0.05697  ...          33.88           123.80      1150.0   \n",
       "11                 0.06082  ...          27.28           136.50      1299.0   \n",
       "12                 0.07800  ...          29.94           151.70      1332.0   \n",
       "13                 0.05338  ...          27.66           112.00       876.5   \n",
       "14                 0.07682  ...          32.01           108.80       697.7   \n",
       "15                 0.07077  ...          37.13           124.10       943.2   \n",
       "16                 0.05922  ...          30.88           123.40      1138.0   \n",
       "17                 0.07356  ...          31.48           136.80      1315.0   \n",
       "18                 0.05395  ...          30.88           186.80      2398.0   \n",
       "19                 0.05766  ...          19.26            99.70       711.2   \n",
       "20                 0.06811  ...          20.49            96.09       630.5   \n",
       "21                 0.06905  ...          15.66            65.13       314.9   \n",
       "22                 0.07032  ...          19.08           125.10       980.9   \n",
       "23                 0.05278  ...          35.59           188.00      2615.0   \n",
       "24                 0.06330  ...          31.56           177.00      2215.0   \n",
       "25                 0.07413  ...          21.40           152.40      1461.0   \n",
       "26                 0.06924  ...          33.21           122.40       896.9   \n",
       "27                 0.05699  ...          27.26           139.90      1403.0   \n",
       "28                 0.06540  ...          36.71           149.30      1269.0   \n",
       "29                 0.06149  ...          19.52           134.90      1227.0   \n",
       "\n",
       "    worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.1622             0.6656          0.71190   \n",
       "1             0.1238             0.1866          0.24160   \n",
       "2             0.1444             0.4245          0.45040   \n",
       "3             0.2098             0.8663          0.68690   \n",
       "4             0.1374             0.2050          0.40000   \n",
       "5             0.1791             0.5249          0.53550   \n",
       "6             0.1442             0.2576          0.37840   \n",
       "7             0.1654             0.3682          0.26780   \n",
       "8             0.1703             0.5401          0.53900   \n",
       "9             0.1853             1.0580          1.10500   \n",
       "10            0.1181             0.1551          0.14590   \n",
       "11            0.1396             0.5609          0.39650   \n",
       "12            0.1037             0.3903          0.36390   \n",
       "13            0.1131             0.1924          0.23220   \n",
       "14            0.1651             0.7725          0.69430   \n",
       "15            0.1678             0.6577          0.70260   \n",
       "16            0.1464             0.1871          0.29140   \n",
       "17            0.1789             0.4233          0.47840   \n",
       "18            0.1512             0.3150          0.53720   \n",
       "19            0.1440             0.1773          0.23900   \n",
       "20            0.1312             0.2776          0.18900   \n",
       "21            0.1324             0.1148          0.08867   \n",
       "22            0.1390             0.5954          0.63050   \n",
       "23            0.1401             0.2600          0.31550   \n",
       "24            0.1805             0.3578          0.46950   \n",
       "25            0.1545             0.3949          0.38530   \n",
       "26            0.1525             0.6643          0.55390   \n",
       "27            0.1338             0.2117          0.34460   \n",
       "28            0.1641             0.6110          0.63350   \n",
       "29            0.1255             0.2812          0.24890   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  class  \n",
       "0                0.26540          0.4601                  0.11890      0  \n",
       "1                0.18600          0.2750                  0.08902      0  \n",
       "2                0.24300          0.3613                  0.08758      0  \n",
       "3                0.25750          0.6638                  0.17300      0  \n",
       "4                0.16250          0.2364                  0.07678      0  \n",
       "5                0.17410          0.3985                  0.12440      0  \n",
       "6                0.19320          0.3063                  0.08368      0  \n",
       "7                0.15560          0.3196                  0.11510      0  \n",
       "8                0.20600          0.4378                  0.10720      0  \n",
       "9                0.22100          0.4366                  0.20750      0  \n",
       "10               0.09975          0.2948                  0.08452      0  \n",
       "11               0.18100          0.3792                  0.10480      0  \n",
       "12               0.17670          0.3176                  0.10230      0  \n",
       "13               0.11190          0.2809                  0.06287      0  \n",
       "14               0.22080          0.3596                  0.14310      0  \n",
       "15               0.17120          0.4218                  0.13410      0  \n",
       "16               0.16090          0.3029                  0.08216      0  \n",
       "17               0.20730          0.3706                  0.11420      0  \n",
       "18               0.23880          0.2768                  0.07615      0  \n",
       "19               0.12880          0.2977                  0.07259      1  \n",
       "20               0.07283          0.3184                  0.08183      1  \n",
       "21               0.06227          0.2450                  0.07773      1  \n",
       "22               0.23930          0.4667                  0.09946      0  \n",
       "23               0.20090          0.2822                  0.07526      0  \n",
       "24               0.20950          0.3613                  0.09564      0  \n",
       "25               0.25500          0.4066                  0.10590      0  \n",
       "26               0.27010          0.4264                  0.12750      0  \n",
       "27               0.14900          0.2341                  0.07421      0  \n",
       "28               0.20240          0.4027                  0.09876      0  \n",
       "29               0.14560          0.2756                  0.07919      0  \n",
       "\n",
       "[30 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "tsUus1iX0uEg",
    "outputId": "64e7b8b6-e953-4fe6-a4f3-513577c4b2a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       worst fractal dimension       class  \n",
       "count               569.000000  569.000000  \n",
       "mean                  0.083946    0.627417  \n",
       "std                   0.018061    0.483918  \n",
       "min                   0.055040    0.000000  \n",
       "25%                   0.071460    0.000000  \n",
       "50%                   0.080040    1.000000  \n",
       "75%                   0.092080    1.000000  \n",
       "max                   0.207500    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbGT5rn30zv3",
    "outputId": "304562c4-70f1-4282-e2b5-7c00f589cd2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    357\n",
      "0    212\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeZLGm-c0__F",
    "outputId": "07395de6-4235-486a-cff6-7632f1ae23aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "mhvwBVWv1Ild",
    "outputId": "157665f8-f270-4148-a1e0-32140b7a98ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.462830</td>\n",
       "      <td>21.604906</td>\n",
       "      <td>115.365377</td>\n",
       "      <td>978.376415</td>\n",
       "      <td>0.102898</td>\n",
       "      <td>0.145188</td>\n",
       "      <td>0.160775</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.192909</td>\n",
       "      <td>0.062680</td>\n",
       "      <td>...</td>\n",
       "      <td>21.134811</td>\n",
       "      <td>29.318208</td>\n",
       "      <td>141.370330</td>\n",
       "      <td>1422.286321</td>\n",
       "      <td>0.144845</td>\n",
       "      <td>0.374824</td>\n",
       "      <td>0.450606</td>\n",
       "      <td>0.182237</td>\n",
       "      <td>0.323468</td>\n",
       "      <td>0.091530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.146524</td>\n",
       "      <td>17.914762</td>\n",
       "      <td>78.075406</td>\n",
       "      <td>462.790196</td>\n",
       "      <td>0.092478</td>\n",
       "      <td>0.080085</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.025717</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.062867</td>\n",
       "      <td>...</td>\n",
       "      <td>13.379801</td>\n",
       "      <td>23.515070</td>\n",
       "      <td>87.005938</td>\n",
       "      <td>558.899440</td>\n",
       "      <td>0.124959</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.166238</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.270246</td>\n",
       "      <td>0.079442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter   mean area  mean smoothness  \\\n",
       "class                                                                           \n",
       "0        17.462830     21.604906      115.365377  978.376415         0.102898   \n",
       "1        12.146524     17.914762       78.075406  462.790196         0.092478   \n",
       "\n",
       "       mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "class                                                                         \n",
       "0              0.145188        0.160775             0.087990       0.192909   \n",
       "1              0.080085        0.046058             0.025717       0.174186   \n",
       "\n",
       "       mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "class                          ...                                \n",
       "0                    0.062680  ...     21.134811      29.318208   \n",
       "1                    0.062867  ...     13.379801      23.515070   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "class                                                                      \n",
       "0           141.370330  1422.286321          0.144845           0.374824   \n",
       "1            87.005938   558.899440          0.124959           0.182673   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "class                                                          \n",
       "0             0.450606              0.182237        0.323468   \n",
       "1             0.166238              0.074444        0.270246   \n",
       "\n",
       "       worst fractal dimension  \n",
       "class                           \n",
       "0                     0.091530  \n",
       "1                     0.079442  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('class').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwXTD0Ic1pgF"
   },
   "source": [
    "**Class Labels**\n",
    "\n",
    "0 - *Malignant*\n",
    "\n",
    "1 - *Benign*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK93FPag1_V8"
   },
   "source": [
    "Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bbpxFuo21eIo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2XBNmwkO2Muw"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPWgzQjx2azS",
    "outputId": "bf377b28-37d0-49fa-a8c2-d992bc5e7eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,) (426,) (143,)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HKtuzGb82f2R"
   },
   "outputs": [],
   "source": [
    "# test size - to specify the percentage of test data needed\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Megfcg992wh8",
    "outputId": "2218e860-cda2-4d89-ef43-730667fc78d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,) (455,) (114,)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "224kJgg72yM3",
    "outputId": "ff45d0ca-dca2-47c4-e6e6-4fa4243e4916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274165202108963 0.6395604395604395 0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "print(Y.mean(),Y_train.mean(),Y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwsZT-CV2_oA",
    "outputId": "672960a5-30ca-45c6-c6b9-9a44da624d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274165202108963 0.6263736263736264 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "# startify - split the equal distribution(balance number) of examples for each class label\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y)\n",
    "print(Y.mean(),Y_train.mean(),Y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehkgWyZg3Ywy",
    "outputId": "879d4c36-bbc6-4e08-9c7c-eb081d9e28dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274165202108963 0.6263736263736264 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "# random_state - specific split of data. Each value of random_state splits the data differently\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y, random_state=1)\n",
    "print(Y.mean(),Y_train.mean(),Y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcMQqvii7x7X",
    "outputId": "d97167f4-742e-4449-b87c-c082bc354a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.04079777197802 65.28291521874269 61.890712339519624\n"
     ]
    }
   ],
   "source": [
    "print(X_train.mean(),X_test.mean(),X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOEV_qonmzF3",
    "outputId": "e216452c-5385-4429-a75b-879f4167e7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.422e+01 2.312e+01 9.437e+01 ... 1.772e-01 5.166e-01 1.446e-01]\n",
      " [1.846e+01 1.852e+01 1.211e+02 ... 1.642e-01 3.695e-01 8.579e-02]\n",
      " [1.340e+01 2.052e+01 8.864e+01 ... 2.051e-01 3.585e-01 1.109e-01]\n",
      " ...\n",
      " [1.152e+01 1.493e+01 7.387e+01 ... 9.608e-02 2.664e-01 7.809e-02]\n",
      " [1.218e+01 1.408e+01 7.725e+01 ... 1.852e-02 2.293e-01 6.037e-02]\n",
      " [1.919e+01 1.594e+01 1.263e+02 ... 1.777e-01 2.443e-01 6.251e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLDyeFdznQSI"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qWPRXRTNnKQ_"
   },
   "outputs": [],
   "source": [
    "# import Logistic Regression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NBucPJDznqym"
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression() # loading the logistic regression model to the variable \"classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3jG2iYDn2ha",
    "outputId": "bba1bd43-e4ca-47df-8c6e-75a17818504d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model on training data\n",
    "classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCjUOhnBoJiP"
   },
   "source": [
    "**Evaluation of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ssYYfkNXoDaW"
   },
   "outputs": [],
   "source": [
    "# import accuracy_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9g-cT9TnoT-1"
   },
   "outputs": [],
   "source": [
    "prediction_on_training_data = classifier.predict(X_train)\n",
    "accuracy_on_training_data = accuracy_score(Y_train, prediction_on_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zyo_9T8jogyE",
    "outputId": "632627a8-6261-43f5-f845-c83c9a327280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data =  0.9516483516483516\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Training Data = \",accuracy_on_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xToL6Uviom5Y"
   },
   "outputs": [],
   "source": [
    "# predict on test_data\n",
    "prediction_on_test_data = classifier.predict(X_test)\n",
    "accuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqMC7OMto9ne",
    "outputId": "c8657760-cd9b-47b2-f32c-f221bcb9b329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data =  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Test Data = \",accuracy_on_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pRMGmyapSAH"
   },
   "source": [
    "**Detecting whether the Patient has breast cancer in Bening or Malignant stage using Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLs2sLispGaO",
    "outputId": "085d62a5-8ae9-4c07-f41d-f4215d1a0b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.73, 22.61, 93.6, 578.3, 0.1131, 0.2293, 0.2128, 0.08025, 0.2069, 0.07682, 0.2121, 1.169, 2.061, 19.21, 0.006429, 0.05936, 0.05501, 0.01628, 0.01961, 0.008093, 15.03, 32.01, 108.8, 697.7, 0.1651, 0.7725, 0.6943, 0.2208, 0.3596, 0.1431)\n",
      "[0]\n",
      "The Breast Cancer is Malignant\n"
     ]
    }
   ],
   "source": [
    "# Actual output for the input data is 0(Malignant)\n",
    "input_data = (13.73,22.61,93.6,578.3,0.1131,0.2293,0.2128,0.08025,0.2069,0.07682,0.2121,1.169,2.061,19.21,0.006429,0.05936,0.05501,0.01628,0.01961,0.008093,15.03,32.01,108.8,697.7,0.1651,0.7725,0.6943,0.2208,0.3596,0.1431)\n",
    "\n",
    "# change the input_data to numpy_array to make prediction\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "print(input_data)\n",
    "\n",
    "# reshape the array as we are predicting the output for one instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "# prediction\n",
    "prediction = classifier.predict(input_data_reshaped)\n",
    "print(prediction) # returns a list with element [0] if Malignant; returns a list with element [1] if benign.\n",
    "\n",
    "if (prediction == [0]):\n",
    "  print(\"The Breast Cancer is Malignant\")\n",
    "else:\n",
    "  print(\"The Breast Cancer is Malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGO35r2-sDRo",
    "outputId": "181d5d77-322f-4e0b-d55f-118d3e35f5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.504, 12.44, 60.34, 273.9, 0.1024, 0.06492, 0.02956, 0.02076, 0.1815, 0.06905, 0.2773, 0.9768, 1.909, 15.7, 0.009606, 0.01432, 0.01985, 0.01421, 0.02027, 0.002968, 10.23, 15.66, 65.13, 314.9, 0.1324, 0.1148, 0.08867, 0.06227, 0.245, 0.07773)\n",
      "[1]\n",
      "The Breast Cancer is Benign\n"
     ]
    }
   ],
   "source": [
    "# Actual output for the input data is 1(Benign)\n",
    "input_data = (9.504,12.44,60.34,273.9,0.1024,0.06492,0.02956,0.02076,0.1815,0.06905,0.2773,0.9768,1.909,15.7,0.009606,0.01432,0.01985,0.01421,0.02027,0.002968,10.23,15.66,65.13,314.9,0.1324,0.1148,0.08867,0.06227,0.245,0.07773)\n",
    "\n",
    "# change the input_data to numpy_array to make prediction\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "print(input_data)\n",
    "\n",
    "# reshape the array as we are predicting the output for one instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "# prediction\n",
    "prediction = classifier.predict(input_data_reshaped)\n",
    "print(prediction) # returns a list with element [0] if Malignant; returns a list with element [1] if benign.\n",
    "\n",
    "if (prediction == [0]):\n",
    "  print(\"The Breast Cancer is Malignant\")\n",
    "else:\n",
    "  print(\"The Breast Cancer is Benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFp9zOIVeIAW"
   },
   "source": [
    "**KNN - K Nearest Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JpAkdvptMEf",
    "outputId": "81e44a7d-8905-448d-f764-29006e2f8419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy :  0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# model : KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2) #increase in k value increases the accuracy\n",
    "# fit : training model\n",
    "knn.fit(X_train,Y_train)\n",
    "prediction_on_train_data = knn.predict(X_train)\n",
    "#import metrics for accuracy calculation\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(Y_train,prediction_on_train_data)\n",
    "print(\"Acuuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-JzyREBfiIl",
    "outputId": "d3d16319-6273-4af8-f84f-af42e34a3d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy :  0.9472527472527472\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=4) #increase in k value increases the accuracy\n",
    "# fit : training model\n",
    "knn.fit(X_train,Y_train)\n",
    "prediction_on_train_data = knn.predict(X_train)\n",
    "#import metrics for accuracy calculation\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(Y_train,prediction_on_train_data)\n",
    "print(\"Acuuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kl_Xopr6gVm3",
    "outputId": "ef174ae4-9276-40ef-be7e-9cd493d1680b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy :  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2) #increase in k value increases the accuracy\n",
    "# fit : training model\n",
    "knn.fit(X_train,Y_train)\n",
    "prediction_on_test_data = knn.predict(X_test)\n",
    "#import metrics for accuracy calculation\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(Y_test,prediction_on_test_data)\n",
    "print(\"Acuuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6NnTl08ibLb",
    "outputId": "4b743ffb-7bbc-4488-91aa-48d5690c3f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy :  0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=4) #increase in k value increases the accuracy\n",
    "# fit : training model\n",
    "knn.fit(X_train,Y_train)\n",
    "prediction_on_test_data = knn.predict(X_test)\n",
    "#import metrics for accuracy calculation\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(Y_test,prediction_on_test_data)\n",
    "print(\"Acuuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inEQR_YYlEYo"
   },
   "source": [
    "**Detecting whether the Patient has breast cancer in Bening or Malignant stage using KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FsuAbv-iwhW",
    "outputId": "18ab2503-4474-4e8b-912b-552662c94bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19.81, 22.15, 130, 1260, 0.09831, 0.1027, 0.1479, 0.09498, 0.1582, 0.05395, 0.7582, 1.017, 5.865, 112.4, 0.006494, 0.01893, 0.03391, 0.01521, 0.01356, 0.001997, 27.32, 30.88, 186.8, 2398, 0.1512, 0.315, 0.5372, 0.2388, 0.2768, 0.07615)\n",
      "[0]\n",
      "The Breast Cancer is Malignant\n"
     ]
    }
   ],
   "source": [
    "# Actual output for the input data is 0(Malignant)\n",
    "input_data = (19.81,22.15,130,1260,0.09831,0.1027,0.1479,0.09498,0.1582,0.05395,0.7582,1.017,5.865,112.4,0.006494,0.01893,0.03391,0.01521,0.01356,0.001997,27.32,30.88,186.8,2398,0.1512,0.315,0.5372,0.2388,0.2768,0.07615)\n",
    "\n",
    "# change the input_data to numpy_array to make prediction\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "print(input_data)\n",
    "\n",
    "# reshape the array as we are predicting the output for one instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "# prediction\n",
    "prediction = knn.predict(input_data_reshaped)\n",
    "print(prediction) # returns a list with element [0] if Malignant; returns a list with element [1] if benign.\n",
    "\n",
    "if (prediction == [0]):\n",
    "  print(\"The Breast Cancer is Malignant\")\n",
    "else:\n",
    "  print(\"The Breast Cancer is Malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "On6hpGGdlalZ",
    "outputId": "5ab62978-6552-4ba3-a2dd-83aad0f430ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.888, 14.64, 58.79, 244, 0.09783, 0.1531, 0.08606, 0.02872, 0.1902, 0.0898, 0.5262, 0.8522, 3.168, 25.44, 0.01721, 0.09368, 0.05671, 0.01766, 0.02541, 0.02193, 9.733, 15.67, 62.56, 284.4, 0.1207, 0.2436, 0.1434, 0.04786, 0.2254, 0.1084)\n",
      "[1]\n",
      "The Breast Cancer is Benign\n"
     ]
    }
   ],
   "source": [
    "# Actual output for the input data is 1(Benign)\n",
    "input_data = (8.888,14.64,58.79,244,0.09783,0.1531,0.08606,0.02872,0.1902,0.0898,0.5262,0.8522,3.168,25.44,0.01721,0.09368,0.05671,0.01766,0.02541,0.02193,9.733,15.67,62.56,284.4,0.1207,0.2436,0.1434,0.04786,0.2254,0.1084)\n",
    "\n",
    "# change the input_data to numpy_array to make prediction\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "print(input_data)\n",
    "\n",
    "# reshape the array as we are predicting the output for one instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "# prediction\n",
    "prediction = knn.predict(input_data_reshaped)\n",
    "print(prediction) # returns a list with element [0] if Malignant; returns a list with element [1] if benign.\n",
    "\n",
    "if (prediction == [0]):\n",
    "  print(\"The Breast Cancer is Malignant\")\n",
    "else:\n",
    "  print(\"The Breast Cancer is Benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZgdqsLlRLEW"
   },
   "source": [
    "**Compare Logistic Regression and KNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IZQ2EH2dlejR"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cvEMRbfRljf",
    "outputId": "10e1cf19-bc93-4ff3-8d80-b7643013bb10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LR',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('KNN',\n",
       "  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                       weights='uniform'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Logistic Regression and KNN model\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUQ_ZFDBRyuz",
    "outputId": "6aa1da7d-faec-44fb-9606-887be27ac9f2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.944976 (0.030038)\n",
      "KNN: 0.923140 (0.026272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\VAISHU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the models\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=5)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "7rqngOk0SYCY",
    "outputId": "81f6d672-9c14-4a83-81da-f0dafc70feb0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting BoxPlot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.suptitle('Comparison of Logistic Regression(LR) and KNN')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Cross Validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxVvw994S9eb"
   },
   "outputs": [],
   "source": [
    "# Logistric Regression is better than KNN model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Breast Cancer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
